---
title: "CSE160 Project"
author: "Hamza Ali"
date: "5/2/2022"
output: html_document
---


## Setting up the table

```{r}
library(ggplot2)
data <- read.csv("https://raw.githubusercontent.com/srm322/CSE160FinalProject/main/CombinedData.csv") 
data <- data[,-c(21,22,23,24,25)]
data <- na.omit(data)
hist(data$NumFirearmFatalities[data$NumFirearmFatalities < 1000]) #
data$dangerous <- data$NumFirearmFatalities > 200
data$extreme_dangerous <- data$NumFirearmFatalities > 2000
```

## Making the models
NOTE: THESE ARE THE VARIABLES IN THE TABLE (AIAN,Asian,Black,Hispanic,White were taken out for a lot of NA)
STNAME,CTYNAME,POPESTIMATE2019,NPOPCHG_2019,BIRTHS2019,DEATHS2019,NATURALINC2019,INTERNATIONALMIG2019,DOMESTICMIG2019,NETMIG2019,RESIDUAL2019,GQESTIMATES2019,RBIRTH2019,RDEATH2019,RNATURALINC2019,RINTERNATIONALMIG2019,RDOMESTICMIG2019,RNETMIG2019,Personal.income..Thousands.of.dollars. ,NumFirearmFatalities,AVGTotalGuns, dangerous, extreme_dangerous

```{r}
library(caTools)
library(ROCR)
library(e1071)
split <- sample(2, nrow(data), replace = TRUE, prob = c(.8, .2))
trainData <- data[split == 1,]
testData <- data[split == 2,]
#NB
nb <- naiveBayes(formula = as.factor(dangerous) ~ POPESTIMATE2019 +NPOPCHG_2019 +BIRTHS2019 +NATURALINC2019 +INTERNATIONALMIG2019 +DOMESTICMIG2019 +NETMIG2019 +RESIDUAL2019 +GQESTIMATES2019 +RBIRTH2019 +RDEATH2019 +RNATURALINC2019 +RINTERNATIONALMIG2019 +RDOMESTICMIG2019 +RNETMIG2019 +Personal.income..Thousands.of.dollars. +AVGTotalGuns, data = trainData) #+AIAN +Asian +Black +Hispanic +White
pNB <- predict(nb, newdata = testData, type = 'raw')
t1 <- table(data.frame(testData$dangerous, pNB > 0.5))
curveNB <- prediction(pNB[,2], testData$dangerous)
performanceNB <- performance(curveNB, 'tpr', 'fpr')
regressionModel <- glm(dangerous ~ POPESTIMATE2019 +NPOPCHG_2019 +BIRTHS2019 +NATURALINC2019 +INTERNATIONALMIG2019 +DOMESTICMIG2019 +NETMIG2019 +RESIDUAL2019 +GQESTIMATES2019 +RBIRTH2019 +RDEATH2019 +RNATURALINC2019 +RINTERNATIONALMIG2019 +RDOMESTICMIG2019 +RNETMIG2019 +Personal.income..Thousands.of.dollars. +AVGTotalGuns, data = trainData, family = "binomial")
summary(regressionModel)$coef ##ADDED TO SHOW WHAT THE REGRESSION DID
pred <- predict(regressionModel, testData, type = "response")
t2 <-table(data.frame(testData$dangerous, pred > .5))
curveLR <- prediction(pred, testData$dangerous)
performanceLR <- performance(curveLR, 'tpr', 'fpr')
plot(performanceNB, col = 1, main = "ROC")
plot(performanceLR, col = 2, add = T)
legend(.6,.3,c('Naive Bayes', 'Logistic Regression'), 1:2)
t2
```

## Decision Trees

```{r}
library(rpart)
library(rpart.plot)
library(caret)
#set.seed(4567)
split <- sample(2, nrow(data), replace = TRUE, prob = c(.8, .2))
trainData <- data[split == 1,]
testData <- data[split == 2,]
tree <- rpart(dangerous ~ POPESTIMATE2019 +NPOPCHG_2019 +BIRTHS2019 +NATURALINC2019 +INTERNATIONALMIG2019 +DOMESTICMIG2019 +NETMIG2019 +RESIDUAL2019 +GQESTIMATES2019 +RBIRTH2019 +RDEATH2019 +RNATURALINC2019 +RINTERNATIONALMIG2019 +RDOMESTICMIG2019 +RNETMIG2019 +Personal.income..Thousands.of.dollars. +AVGTotalGuns, data = trainData, method = "class")
prp(tree)

# plots regression
#evaluate tree on training data
tree.train <- predict(tree, newdata=testData, type="class")
#confusionMatrix(forest.train, testData$is_successful)
table.traintree <- table(tree.train, testData$dangerous)
#calculate accuracies
accuracy.traintree = sum(diag(table.traintree))/sum(table.traintree) 
table.traintree
cat("Tree training accuracy: ", accuracy.traintree, "\n")
```


## 10-Fold Cross validation Naive Bayes

```{r}
library(e1071)
#make matrix to store values
recall_totals <- 0
precision_totals <- 0
accuracy_totals <- 0
fMeasure_totals <- 0
#10-fold validation using Naive Bayes
for (i in 1:10) {
  m <- dim(data)[1]
  foldLength <- round(m/10) # Create 10 folds
  val <- c(1:foldLength) + foldLength*(i-1)
  if (i == 10) {
    val <- c(m-foldLength:m)
  }
  trainData <- data[-val, ]
  testData <- data[val, ]
    
#use naive bayes to create classifier model
  nb <- naiveBayes(dangerous~POPESTIMATE2019 +NPOPCHG_2019 +BIRTHS2019 +NATURALINC2019 +INTERNATIONALMIG2019 +DOMESTICMIG2019 +NETMIG2019 +RESIDUAL2019 +GQESTIMATES2019 +RBIRTH2019 +RDEATH2019 +RNATURALINC2019 +RINTERNATIONALMIG2019 +RDOMESTICMIG2019 +RNETMIG2019 +Personal.income..Thousands.of.dollars. +AVGTotalGuns, data = trainData)
  pred <- predict(nb, newdata = testData)
  tab <- table(testData$dangerous, pred)
  tab
#define true and false positives and negatives
  TP <- tab[1,1]
  FP <- tab[1,2]
  FN <- tab[2,1]
  TN <- tab[2,2]
    
#calculate precision, recall, accuracy, and f-measure respectively, for the fold
  precision <- TP/(TP+FP)
  precision_totals <- precision_totals + precision
    
  recall <- TP/(TP+FN)
  recall_totals <- recall_totals + recall
   
  accuracy <- (TP+TN)/(TP+TN+FP+FN)
  accuracy_totals <- accuracy_totals + accuracy
  fMeasure <- 2 * precision * recall / (precision + recall)
  fMeasure_totals <- fMeasure_totals + fMeasure
}
#Average measures and print
cat("\nAverage Accuracy:\n")
accuracy_totals/10
cat("\nAverage Precision:\n")
precision_totals/10
cat("\nAverage Recall:\n")
recall_totals/10
cat("\nAverage f-Measure:\n")
fMeasure_totals/10
```
